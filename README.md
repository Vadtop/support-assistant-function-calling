# Support Assistant with Function Calling

AI support assistant with function calling for fintech use case. LLM autonomously decides when to use FAQ search or loan calculator tools.

## ğŸ¯ Project Goal

Production-ready AI support assistant demonstrating function calling â€” 
LLM autonomously invokes external tools (FAQ search, loan calculator) 
based on user query.

## ğŸ—ï¸ Architecture

```
User Question
    â†“
LLM (GPT-4o-mini) + tools definition
    â†“
[Decision: use tool?]
    â†“
YES â†’ execute tool (search_faq / calculate_loan)
    â†“
Tool results â†’ LLM
    â†“
Final answer based on real data
```

## âœ¨ Features

- **Function calling**: LLM automatically invokes `search_faq()` or `calculate_loan()` when needed
- **Smart decision making**: Model chooses between tools or direct response
- **Multiple tools**: FAQ search + loan calculator in one assistant
- **Token-based FAQ search**: Simple but effective (easy to scale to vector DB)
- **Loan calculator**: Extract parameters from natural language and calculate payments
- **FastAPI**: Production-ready API with auto-documentation
- **Fintech scenario**: FAQ about cards, limits, transfers, balance, cashback

## ğŸ› ï¸ Tech Stack

- **Python 3.11**
- **FastAPI** â€” modern web framework for APIs
- **OpenAI API** (via OpenRouter) â€” LLM with function calling
- **Token-based search** â€” simple yet effective FAQ search

## ğŸ“¦ Installation

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/support-assistant-function-calling.git
cd support-assistant-function-calling

# Create virtual environment
python -m venv venv

# Windows:
venv\Scripts\activate

# Linux / macOS:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Set up API key
# Create .env file in project root:
echo "OPENROUTER_API_KEY=your-key-here" > .env
```

## ğŸš€ Usage

### Start API server

```bash
uvicorn app.main:app --reload
```

Open interactive docs at: http://localhost:8000/docs

### Example Requests

#### FAQ Search

```bash
curl -X POST "http://localhost:8000/support/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "How can I check my card balance?"}'
```

Response:

```json
{
  "assistant_answer": "You can check balance in mobile app...",
  "tool_used": true,
  "tool_name": "search_faq",
  "faq_results": [...]
}
```

#### Loan Calculation

```bash
curl -X POST "http://localhost:8000/support/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "I want a loan of 500,000 rubles for 12 months at 15%"}'
```

Response:

```json
{
  "assistant_answer": "Monthly payment: 45,129 rubles...",
  "tool_used": true,
  "tool_name": "calculate_loan",
  "calculation_result": {
    "monthly_payment": 45129.16,
    "total_payment": 541549.87,
    "overpayment": 41549.87
  }
}
```

#### General Question (no tool)

```bash
curl -X POST "http://localhost:8000/support/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "Thank you!"}'
```

Response:

```json
{
  "assistant_answer": "You're welcome! Let me know if you need help.",
  "tool_used": false
}
```

## ğŸŒ API Endpoints

### `GET /`
Health check endpoint.

### `GET /faq/search`
Test FAQ search without LLM.
- Parameters: `q` (search query), `top_k` (number of results)

### `POST /support/chat`
Main assistant endpoint with function calling.
- Request body: `{"message": "your question here"}`

## ğŸ“‚ Project Structure

```
support-assistant-function-calling/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py           # FastAPI app + endpoints
â”‚   â”œâ”€â”€ faq_data.py       # Knowledge base (12 FAQ items)
â”‚   â””â”€â”€ tools.py          # Tools: search_faq + calculate_loan
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ conversations.md  # Dialog examples
â”œâ”€â”€ .env                  # API keys (not in git)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”§ Technical Highlights

### Core Concepts

- **Function calling** â€” LLM can invoke external functions/APIs for data retrieval
- **Tool decision making** â€” model autonomously decides when to use tools
- **Parameter extraction** â€” LLM extracts structured data from natural language
- **Multiple tools** â€” one assistant can handle different types of requests

### Implementation Details

- Tool definition via JSON schema (function name, description, parameters)
- Two-step flow: (1) LLM decides â†’ (2) execute tool â†’ (3) LLM formulates answer
- `tool_choice="auto"` â€” model decides when to use tool vs direct response
- Handling multiple tools with if/elif branches

## ğŸ“Š Project Evolution

### Current Version (v1.0)

- âœ… Function calling with 2 tools (FAQ search + loan calculator)
- âœ… FastAPI REST API
- âœ… Token-based FAQ search (12 items)
- âœ… Loan calculator with parameter extraction
- âœ… Examples and documentation

### Planned (v1.1)

- â³ Add vector DB (Pinecone/Weaviate) for FAQ search
- â³ Multi-turn conversations with dialog history
- â³ Third tool: create_ticket for escalation
- â³ Metrics (latency, tool usage rate)

### Future (v2.0)

- ğŸ”® Deploy to Railway/Render
- ğŸ”® Add monitoring and logging
- ğŸ”® A/B testing (with tool vs without tool)

## ğŸ’¡ Why This Approach?

Starting with simple token-based search and manual tool handling, then scaling to vector DB and advanced orchestration. This gives:

- Clear understanding of function calling internals
- Ability to debug tool invocation issues

## ğŸ“§ Contact

GitHub: [@Vadtop](https://github.com/Vadtop)
